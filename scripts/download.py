#!/usr/bin/env python3
import argparse
import logging
import os
import sys
from concurrent import futures
from pathlib import Path

import requests

from saadt.model import Paper, parse_config
from saadt.util import get_proxy
from saadt.util.log import get_logger
from saadt.util.session import create_session


def download_link(session: requests.Session, link: str, target: str) -> None:
    with session.get(link, stream=True) as r:
        r.raise_for_status()

        if "application/pdf" not in r.headers["Content-Type"]:
            raise ValueError(f"Content-Type is not application/pdf: {r.headers['Content-Type']}")

        # let's assume it's a pdf :)
        path = Path(target)
        # always overwrite
        with path.open(mode="wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)


def download_paper(logger: logging.Logger, session: requests.Session, dest_dir: str, paper: Paper) -> None:
    assert paper.pdf_link is not None
    logger.info(f'Downloading paper "{paper.title}" from "{paper.pdf_link}"')
    try:
        download_link(session, paper.pdf_link, os.path.join(os.path.abspath(dest_dir), f"{paper.id()}.pdf"))
    except requests.HTTPError as err:
        logging.error("Failed downloading paper %s: %s", paper.title, err)

    if paper.appendix_link is not None and paper.appendix_link.endswith(".pdf"):
        try:
            download_link(
                session, paper.appendix_link, os.path.join(os.path.abspath(dest_dir), f"{paper.id()}_appendix.pdf")
            )
        except requests.HTTPError as err:
            logging.warning("Failed downloading appendix %s: %s", paper.title, err)


def download_papers(logger: logging.Logger, dest_dir: str, papers: list[Paper]) -> None:
    session = create_session(proxies=(get_proxy()))
    session.headers.update(
        {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.3"
        }
    )

    executor = futures.ThreadPoolExecutor(max_workers=2, thread_name_prefix="worker")
    executor.map(lambda paper: download_paper(logger, session, dest_dir, paper), papers)
    executor.shutdown()


def main() -> None:
    parser = argparse.ArgumentParser(
        description="""Conference paper downloader
        Uses a json file generated by scrape.py to download the papers."""
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose operation")
    parser.add_argument("-q", "--quiet", action="store_true", default=0, help="Disable all logging")
    parser.add_argument("--debug", metavar="[debug file]", help="path to file for debug logging")
    parser.add_argument("config_file_path", metavar="[config file]", help="config file location")
    parser.add_argument("destdir", help="directory to download papers to")
    args = parser.parse_args()

    level = logging.INFO
    if args.verbose:
        level = logging.DEBUG
    if args.quiet:
        level = logging.ERROR
    log = get_logger(level, args.debug)

    data = parse_config(args.config_file_path)

    log.info(
        f"Downloading {len(data['papers'])} papers of {data['conference']} 20{data['edition']} to {os.path.join(os.getcwd(), args.destdir)}"
    )

    download_papers(log, args.destdir, data["papers"])
    log.info("Finished")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt as e:
        logging.info(f'{type(e).__name__}: {"Terminated."}')
        sys.exit(1)
